{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real or Not? NLP with Disaster Tweets   \n",
    "Predict which Tweets are about real disasters and which ones are not\n",
    "\n",
    "environment:   \n",
    "tensorflow 2.2.0  \n",
    "karas 2.3.1   \n",
    "sklearn 0.22.1   \n",
    "platform: \n",
    "I7-9700k GTX-1080ti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data\n",
    "../train.csv                  7613   \n",
    "../test.csv                   3263   \n",
    "../sample_submission.csv      3263   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5) (3263, 4) (3263, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../train.csv\")\n",
    "test_df = pd.read_csv(\"../test.csv\")\n",
    "sub_df = pd.read_csv(\"../sample_submission.csv\")\n",
    "print (train_df.shape, test_df.shape, sub_df.shape)\n",
    "# (7613, 5) (3263, 4) (3263, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is no duplicated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The train set distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = train_df.drop_duplicates().reset_index(drop=True)\n",
    "sns.countplot(y=train_df.target);\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the train.csv into 80% train data and 20% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_df[\"text\"]\n",
    "y = train_df[\"target\"]\n",
    "\n",
    "\n",
    "# Separate the train.csv into 80% train data and 20% test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4996    Courageous and honest analysis of need to use ...\n",
       "3263    @ZachZaidman @670TheScore wld b a shame if tha...\n",
       "4907    Tell @BarackObama to rescind medals of 'honor'...\n",
       "2855    Worried about how the CA drought might affect ...\n",
       "4716    @YoungHeroesID Lava Blast &amp; Power Red #Pan...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stopword in sklearn.   \n",
    "CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stopword in sklearn.\n",
    "vect = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "x_train_cv = vect.fit_transform(X_train)\n",
    "x_test_cv = vect.transform(X_test)\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn.naive_bayes.MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(x_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7872619829284307"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output \"submission_bayes.csv\"\n",
    "y_test = test_df[\"text\"]\n",
    "y_test_cv = vect.transform(y_test)\n",
    "preds = clf.predict(y_test_cv)\n",
    "sub_df[\"target\"] = preds\n",
    "sub_df.to_csv(\"submission_bayes.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bi-LSTM  \n",
    "Using glove-twitter-27B vector representation of word.  \n",
    "Bi-LSTM model\n",
    "new version of tensorflow should use layers in tensorflow.keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import (LSTM, \n",
    "                          Embedding, \n",
    "                          BatchNormalization,\n",
    "                          Dense, \n",
    "                          TimeDistributed, \n",
    "                          Dropout, \n",
    "                          Bidirectional,\n",
    "                          Flatten, \n",
    "                          GlobalMaxPool1D)\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# new version of tensorflow should use layers in tensorflow.keras \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    classification_report,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.text.values\n",
    "test = test_df.text.values\n",
    "sentiments = train_df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(train)\n",
    "vocab_length = len(word_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pred_tag, y_test):\n",
    "\n",
    "    print(\"F1-score: \", f1_score(pred_tag, y_test))\n",
    "    print(\"Precision: \", precision_score(pred_tag, y_test))\n",
    "    print(\"Recall: \", recall_score(pred_tag, y_test))\n",
    "    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n",
    "    print(\"-\"*50)\n",
    "    print(classification_report(pred_tag, y_test))\n",
    "    \n",
    "def embed(corpus): \n",
    "    return word_tokenizer.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_train = max(train, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "padded_sentences = pad_sequences(embed(train), length_long_sentence, padding='post')\n",
    "\n",
    "test_sentences = pad_sequences(\n",
    "    embed(test), \n",
    "    length_long_sentence,\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove.twitter.27B-100d.txt\n",
    "Pre-trained word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Twitter Gloves\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "embedding_dim = 200\n",
    "# glove_file = open('../input/glove-global-vectors-for-word-representation/glove.6B.' + str(embedding_dim) + 'd.txt', encoding=\"utf8\")\n",
    "glove_file = open('../../glove.twitter.27B.' + str(embedding_dim) + 'd.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    if index >= vocab_length:\n",
    "        continue\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=embedding_matrix.shape[0], \n",
    "                        output_dim=embedding_matrix.shape[1], \n",
    "                        weights = [embedding_matrix], \n",
    "                        input_length=length_long_sentence,\n",
    "                        trainable=False))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(length_long_sentence, return_sequences = True, recurrent_dropout=0.2)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(length_long_sentence, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(length_long_sentence, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Modelo: 0\n",
      "\n",
      "Train on 3806 samples, validate on 3807 samples\n",
      "Epoch 1/15\n",
      "3806/3806 [==============================] - 22s 6ms/step - loss: 0.7081 - accuracy: 0.6432 - val_loss: 0.6185 - val_accuracy: 0.6961\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61854, saving model to model_0.h5\n",
      "Epoch 2/15\n",
      "3806/3806 [==============================] - 21s 5ms/step - loss: 0.5423 - accuracy: 0.7520 - val_loss: 0.5268 - val_accuracy: 0.7896\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61854 to 0.52679, saving model to model_0.h5\n",
      "Epoch 3/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4821 - accuracy: 0.7909 - val_loss: 0.4769 - val_accuracy: 0.8019\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52679 to 0.47691, saving model to model_0.h5\n",
      "Epoch 4/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4666 - accuracy: 0.8022 - val_loss: 0.4745 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.47691 to 0.47448, saving model to model_0.h5\n",
      "Epoch 5/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4343 - accuracy: 0.8234 - val_loss: 0.4209 - val_accuracy: 0.8098\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.47448 to 0.42088, saving model to model_0.h5\n",
      "Epoch 6/15\n",
      "3806/3806 [==============================] - 21s 5ms/step - loss: 0.4142 - accuracy: 0.8242 - val_loss: 0.4310 - val_accuracy: 0.8033\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.42088\n",
      "Epoch 7/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3995 - accuracy: 0.8334 - val_loss: 0.4263 - val_accuracy: 0.8088\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.42088\n",
      "Epoch 8/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3902 - accuracy: 0.8387 - val_loss: 0.4179 - val_accuracy: 0.8140\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.42088 to 0.41791, saving model to model_0.h5\n",
      "Epoch 9/15\n",
      "3806/3806 [==============================] - 21s 5ms/step - loss: 0.3543 - accuracy: 0.8565 - val_loss: 0.4452 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.41791\n",
      "Epoch 10/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3497 - accuracy: 0.8607 - val_loss: 0.4407 - val_accuracy: 0.8051\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.41791\n",
      "Epoch 11/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3311 - accuracy: 0.8655 - val_loss: 0.4490 - val_accuracy: 0.8072\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.41791\n",
      "Epoch 12/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3187 - accuracy: 0.8676 - val_loss: 0.4763 - val_accuracy: 0.8130\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.41791\n",
      "Epoch 13/15\n",
      "3806/3806 [==============================] - 21s 5ms/step - loss: 0.2939 - accuracy: 0.8815 - val_loss: 0.4927 - val_accuracy: 0.8022\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.41791\n",
      "Epoch 14/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2780 - accuracy: 0.8886 - val_loss: 0.4892 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.41791\n",
      "Epoch 15/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2671 - accuracy: 0.9012 - val_loss: 0.5436 - val_accuracy: 0.8075\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.41791\n",
      "********************\n",
      "Modelo: 1\n",
      "\n",
      "Train on 3806 samples, validate on 3807 samples\n",
      "Epoch 1/15\n",
      "3806/3806 [==============================] - 21s 6ms/step - loss: 0.7453 - accuracy: 0.6358 - val_loss: 0.6182 - val_accuracy: 0.7439\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61820, saving model to model_1.h5\n",
      "Epoch 2/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.5471 - accuracy: 0.7425 - val_loss: 0.5168 - val_accuracy: 0.7993\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61820 to 0.51678, saving model to model_1.h5\n",
      "Epoch 3/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4913 - accuracy: 0.7838 - val_loss: 0.4666 - val_accuracy: 0.8119\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51678 to 0.46655, saving model to model_1.h5\n",
      "Epoch 4/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4555 - accuracy: 0.8064 - val_loss: 0.4362 - val_accuracy: 0.8059\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.46655 to 0.43624, saving model to model_1.h5\n",
      "Epoch 5/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4308 - accuracy: 0.8213 - val_loss: 0.4352 - val_accuracy: 0.8127\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.43624 to 0.43523, saving model to model_1.h5\n",
      "Epoch 6/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4033 - accuracy: 0.8305 - val_loss: 0.4193 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43523 to 0.41933, saving model to model_1.h5\n",
      "Epoch 7/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3862 - accuracy: 0.8384 - val_loss: 0.4305 - val_accuracy: 0.8177\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.41933\n",
      "Epoch 8/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3732 - accuracy: 0.8447 - val_loss: 0.4419 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.41933\n",
      "Epoch 9/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3479 - accuracy: 0.8560 - val_loss: 0.4360 - val_accuracy: 0.8156\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.41933\n",
      "Epoch 10/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3328 - accuracy: 0.8665 - val_loss: 0.4688 - val_accuracy: 0.8082\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.41933\n",
      "Epoch 11/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3288 - accuracy: 0.8642 - val_loss: 0.4541 - val_accuracy: 0.8148\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.41933\n",
      "Epoch 12/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2897 - accuracy: 0.8820 - val_loss: 0.5181 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.41933\n",
      "Epoch 13/15\n",
      "3806/3806 [==============================] - 21s 5ms/step - loss: 0.2924 - accuracy: 0.8799 - val_loss: 0.4843 - val_accuracy: 0.8164\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.41933\n",
      "Epoch 14/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2761 - accuracy: 0.8862 - val_loss: 0.5099 - val_accuracy: 0.8151\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.41933\n",
      "Epoch 15/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2617 - accuracy: 0.8891 - val_loss: 0.5266 - val_accuracy: 0.8130\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.41933\n",
      "********************\n",
      "Modelo: 2\n",
      "\n",
      "Train on 3806 samples, validate on 3807 samples\n",
      "Epoch 1/15\n",
      "3806/3806 [==============================] - 21s 6ms/step - loss: 0.7274 - accuracy: 0.6424 - val_loss: 0.6116 - val_accuracy: 0.7394\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61164, saving model to model_2.h5\n",
      "Epoch 2/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.5628 - accuracy: 0.7310 - val_loss: 0.5390 - val_accuracy: 0.8048\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61164 to 0.53904, saving model to model_2.h5\n",
      "Epoch 3/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4920 - accuracy: 0.7856 - val_loss: 0.4758 - val_accuracy: 0.7946\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53904 to 0.47579, saving model to model_2.h5\n",
      "Epoch 4/15\n",
      "3806/3806 [==============================] - 21s 5ms/step - loss: 0.4671 - accuracy: 0.7961 - val_loss: 0.4591 - val_accuracy: 0.7943\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.47579 to 0.45906, saving model to model_2.h5\n",
      "Epoch 5/15\n",
      "3806/3806 [==============================] - 21s 5ms/step - loss: 0.4270 - accuracy: 0.8135 - val_loss: 0.4372 - val_accuracy: 0.8130\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.45906 to 0.43716, saving model to model_2.h5\n",
      "Epoch 6/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4218 - accuracy: 0.8124 - val_loss: 0.4407 - val_accuracy: 0.8130\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43716\n",
      "Epoch 7/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3916 - accuracy: 0.8324 - val_loss: 0.4520 - val_accuracy: 0.8117\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43716\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3669 - accuracy: 0.8468 - val_loss: 0.4383 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43716\n",
      "Epoch 9/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3352 - accuracy: 0.8597 - val_loss: 0.4723 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43716\n",
      "Epoch 10/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3362 - accuracy: 0.8586 - val_loss: 0.5195 - val_accuracy: 0.8040\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43716\n",
      "Epoch 11/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3251 - accuracy: 0.8613 - val_loss: 0.4814 - val_accuracy: 0.8098\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.43716\n",
      "Epoch 12/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2959 - accuracy: 0.8778 - val_loss: 0.5821 - val_accuracy: 0.8025\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43716\n",
      "Epoch 13/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2944 - accuracy: 0.8797 - val_loss: 0.5264 - val_accuracy: 0.8146\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43716\n",
      "Epoch 14/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2787 - accuracy: 0.8865 - val_loss: 0.5689 - val_accuracy: 0.8075\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.43716\n",
      "Epoch 15/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2602 - accuracy: 0.8965 - val_loss: 0.5731 - val_accuracy: 0.8101\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.43716\n",
      "********************\n",
      "Modelo: 3\n",
      "\n",
      "Train on 3806 samples, validate on 3807 samples\n",
      "Epoch 1/15\n",
      "3806/3806 [==============================] - 21s 6ms/step - loss: 0.7441 - accuracy: 0.6471 - val_loss: 0.6204 - val_accuracy: 0.7667\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62041, saving model to model_3.h5\n",
      "Epoch 2/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.5634 - accuracy: 0.7417 - val_loss: 0.5508 - val_accuracy: 0.7822\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62041 to 0.55078, saving model to model_3.h5\n",
      "Epoch 3/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4837 - accuracy: 0.7851 - val_loss: 0.4693 - val_accuracy: 0.8059\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55078 to 0.46929, saving model to model_3.h5\n",
      "Epoch 4/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4693 - accuracy: 0.7964 - val_loss: 0.4971 - val_accuracy: 0.7788\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.46929\n",
      "Epoch 5/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4342 - accuracy: 0.8119 - val_loss: 0.4242 - val_accuracy: 0.8117\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.46929 to 0.42420, saving model to model_3.h5\n",
      "Epoch 6/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4275 - accuracy: 0.8339 - val_loss: 0.4239 - val_accuracy: 0.8111\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.42420 to 0.42391, saving model to model_3.h5\n",
      "Epoch 7/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4088 - accuracy: 0.8250 - val_loss: 0.4298 - val_accuracy: 0.8135\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.42391\n",
      "Epoch 8/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3889 - accuracy: 0.8374 - val_loss: 0.4271 - val_accuracy: 0.8164\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.42391\n",
      "Epoch 9/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3644 - accuracy: 0.8497 - val_loss: 0.4326 - val_accuracy: 0.8164\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.42391\n",
      "Epoch 10/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3421 - accuracy: 0.8610 - val_loss: 0.4462 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.42391\n",
      "Epoch 11/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3237 - accuracy: 0.8689 - val_loss: 0.4868 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.42391\n",
      "Epoch 12/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3067 - accuracy: 0.8705 - val_loss: 0.4807 - val_accuracy: 0.8119\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.42391\n",
      "Epoch 13/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3152 - accuracy: 0.8663 - val_loss: 0.4729 - val_accuracy: 0.8174\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.42391\n",
      "Epoch 14/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2796 - accuracy: 0.8841 - val_loss: 0.5284 - val_accuracy: 0.8193\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.42391\n",
      "Epoch 15/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2706 - accuracy: 0.8865 - val_loss: 0.5378 - val_accuracy: 0.8185\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.42391\n",
      "********************\n",
      "Modelo: 4\n",
      "\n",
      "Train on 3806 samples, validate on 3807 samples\n",
      "Epoch 1/15\n",
      "3806/3806 [==============================] - 21s 6ms/step - loss: 0.7407 - accuracy: 0.6356 - val_loss: 0.6360 - val_accuracy: 0.7628\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63598, saving model to model_4.h5\n",
      "Epoch 2/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.5653 - accuracy: 0.7367 - val_loss: 0.5582 - val_accuracy: 0.7956\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63598 to 0.55821, saving model to model_4.h5\n",
      "Epoch 3/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.5012 - accuracy: 0.7759 - val_loss: 0.4884 - val_accuracy: 0.8004\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55821 to 0.48838, saving model to model_4.h5\n",
      "Epoch 4/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4562 - accuracy: 0.8037 - val_loss: 0.4501 - val_accuracy: 0.8064\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48838 to 0.45011, saving model to model_4.h5\n",
      "Epoch 5/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4409 - accuracy: 0.8032 - val_loss: 0.4356 - val_accuracy: 0.8146\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.45011 to 0.43564, saving model to model_4.h5\n",
      "Epoch 6/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4112 - accuracy: 0.8156 - val_loss: 0.4335 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43564 to 0.43351, saving model to model_4.h5\n",
      "Epoch 7/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.4060 - accuracy: 0.8261 - val_loss: 0.4304 - val_accuracy: 0.8103\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.43351 to 0.43036, saving model to model_4.h5\n",
      "Epoch 8/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3924 - accuracy: 0.8355 - val_loss: 0.4343 - val_accuracy: 0.8188\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43036\n",
      "Epoch 9/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3717 - accuracy: 0.8405 - val_loss: 0.4426 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43036\n",
      "Epoch 10/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3520 - accuracy: 0.8529 - val_loss: 0.4664 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43036\n",
      "Epoch 11/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3302 - accuracy: 0.8673 - val_loss: 0.4713 - val_accuracy: 0.8148\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.43036\n",
      "Epoch 12/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3082 - accuracy: 0.8741 - val_loss: 0.4650 - val_accuracy: 0.8206\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43036\n",
      "Epoch 13/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.3024 - accuracy: 0.8728 - val_loss: 0.4940 - val_accuracy: 0.8177\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43036\n",
      "Epoch 14/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2978 - accuracy: 0.8726 - val_loss: 0.4822 - val_accuracy: 0.8169\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.43036\n",
      "Epoch 15/15\n",
      "3806/3806 [==============================] - 20s 5ms/step - loss: 0.2839 - accuracy: 0.8841 - val_loss: 0.4947 - val_accuracy: 0.8130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00015: val_loss did not improve from 0.43036\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.2, \n",
    "    verbose =1, \n",
    "    patience=5,                        \n",
    "    min_lr=0.001\n",
    ")\n",
    "\n",
    "for idx in range(5):\n",
    "    \n",
    "    print(\"*\"*20 + '\\nModelo: ' + str(idx) + '\\n')\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.2, \n",
    "        verbose =1, \n",
    "        patience=5,                        \n",
    "        min_lr=0.001\n",
    "    )\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'model_' + str(idx)+ '.h5', \n",
    "        monitor='val_loss',\n",
    "        mode='auto',\n",
    "        verbose=1,\n",
    "        save_weights_only = True,\n",
    "        save_best_only=True\n",
    "    )\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        padded_sentences, \n",
    "        sentiments, \n",
    "        test_size=0.5\n",
    "    )\n",
    "    \n",
    "    model = BLSTM()\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              batch_size=32,\n",
    "              epochs=15,\n",
    "              validation_data=[X_test, y_test],\n",
    "              callbacks = [reduce_lr, checkpoint],\n",
    "              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 s, sys: 551 ms, total: 19.5 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from glob import glob\n",
    "import scipy\n",
    "\n",
    "x_models = []\n",
    "labels = []\n",
    "\n",
    "# Carregando os Modelos\n",
    "for idx in glob('*.h5'):\n",
    "    model = BLSTM()\n",
    "    model.load_weights(idx)\n",
    "    x_models.append(model)\n",
    "    \n",
    "# Predizendo Classes para o conjunto de Testes\n",
    "for idx in x_models:\n",
    "    preds = idx.predict_classes(test_sentences)\n",
    "    labels.append(preds)\n",
    "\n",
    "#Votando nas classes, baseando na moda estat√≠stica \n",
    "labels = scipy.stats.mode(labels)[0]\n",
    "labels = np.squeeze(labels)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub_df.target = labels\n",
    "print(labels)\n",
    "sub_df.to_csv(\"submission-bilstm.csv\", index=False)\n",
    "sub_df.target.value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. There is a data called \"socialmedia-disaster-tweets-DFE.csv\",  posted by kaggle  \n",
    "This dataset stems from the figure-eight (formally known as Crowdflower). There do have the prediction of the case we are doing. It's cheating so I just leave there but not submit the answer.  \n",
    "https://www.kaggle.com/jannesklaas/disasters-on-social-media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pd.read_csv(\"../socialmedia-disaster-tweets-DFE.csv\", encoding='latin_1')\n",
    "ans['target'] = (ans['choose_one']=='Relevant').astype(int)\n",
    "ans['id'] = ans.index\n",
    "ans = ans[['id', 'target','text']]\n",
    "merged_df = pd.merge(test_df, ans, on='id')\n",
    "sub1 = merged_df[['id', 'target']]\n",
    "sub1.to_csv('submit_socialmedia_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
